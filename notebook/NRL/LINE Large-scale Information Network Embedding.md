## LINE: Large-scale Information Network Embedding

1. Abstract

   论文的研究目标是将大量的信息网络映射到一个低维的向量空间中，这个应用在可视化，节点分类和边预测方面非常的具有优势，大多数现存的网络映射方法并不能有效的扩展到大规模数据集上，论文新提出了 LINE 算法可以应用在各种各样的图结构中，有权图无权图有向图无向图，通过优化精心设计的目标函数和利用新提出的边采样的方法可以有效的提高算法的性能。

2. Introduction

   论文研究的是对大规模的网络的低维映射，这样的低维映射可以有效的应用在其他的方面，比如推荐系统，节点分类，边预测，可视化等等。现存的大量的方法在小数据集上的效果不错但是没有办法扩展到大数据集上。

   论文提出了 LINE 模型，可以有效的扩展到大数据集上，并且精心设计的目标函数可以有效的保存网络的全局和局部信息。局部信息捕获了网络的一阶近似信息(网络内的邻接关系)，全局信息使用二阶近似表示(不是有连接强度决定，由共享邻域决定，如果两个节点拥有相似的领域的话，节点之间可能具有更深层次的相似关系)

   ![](.\photo\1.png)

   如上图所示，6,7之间具有强大的一阶相似性(临边的权值很大)，并且5,6具有相似的邻域或者说具有巨大的二阶相似性虽然并没有一阶相似性，作者希望这种二阶相似性可以补充稀疏的一阶相似性并保留有网络的全局结构(通过精心设计的目标函数实现)。

   但是就算上述所述的目标函数可以找到，但是在实际中优化这样的函数还是非常困难的，主要原因在于目前使用的 SGD 的方法在实际的网络中不太有效，在实际的网络中，边的权值可能会非常大(比如词共现网络中共现次数有可能非常大，这时候容易梯度爆炸)解决这个问题的方法就是通过按照边权值比例的边采样进行优化。

3. Problem Definition

   1. Information Network

      $G=(V,E)$ each $e=(u,v)\backsim w_{uv} > 0$ 无向图 $(u,v)=(v,u)，w_{uv}=w_{vu}$ 有向图的话 $(u,v)\neq (v,u),w_{uv}\neq w_{vu}$ 网络的 embedding 目的在于尽可能的保存网络的性质

   2. 一阶相似性

      通过边 $(u,v)$ 以及 $w_{uv}$ 表示了 $u,v$ 的一阶相似性，如果没有这样的边，一阶相似性是 0

   3. 二阶相似性

      $p_u=(w_{u,1},...,w_{u,|v|})$ 表示了节点 $u$ 的有一阶相似性的节点，二阶相似性定义为 $p_u,p_v$ 之间的相似性，如果 $p_u,p_v$ 都是 0 二阶相似性也是 0

   4. 大规模网络映射

      映射 $v\in V$ 到一个低维空间中 $R^d(d<|V|)$ 并且保留了必要的一阶和二阶相似性 

4. LINE

   1. 一阶相似性

      $p_1(v_i,v_j)=\frac{1}{1 + \exp(-u_i^T\cdot u_j)}$ 其中 $u_i$ 表示的是低维嵌入，实际的一阶相似性信息是 $p(i,j)=\frac{w_{ij}}{\sum_{(i,j)\in E}w_{ij}}$ 使用 KL 散度计算 $p_1,p$ 之间的差距并进行优化，这是一阶相似性的loss 函数，一阶相似性只适用于无向图。

   2. 二阶相似性

      我们首先需要定义一个结点 $v_i$ 和他的上下文节点 $v_j$ 我们描述 $v_i$ 生成上下文(邻接节点)的概率分布是 $p_2(v_j|v_i)=\frac{\exp(u_j^T\cdot u_i)}{\sum_{k=1}^{|V|}\exp(u_k^T\cdot u_i)}$ 对应的实际相似度指标是 $p(v_j|v_i)=\frac{w_{ij}}{d_i},d_i=\sum_{k\in N(i)}w_{ik}$ 最后使用 KL 散度描述两个概率分布的差异并进行优化。

      $O_2=-\sum_{(i,j)\in E} w_{ij}\log p_w(v_j|v_i)$ 

   3. 优化

      比如优化上面的 $O_2$ 的过程中对每一个 $v_i$ 都需要计算所有的 $v$ 。优化过程中为了调高效率使用负采样技术。同理在面对 $O_1$ 的时候也使用负采样但是这时候有可能 $w_{ij}$ 会非常大，解决的方法是对边进行采样。



​       
